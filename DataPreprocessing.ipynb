{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6f0fa278",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f11f7c24",
   "metadata": {},
   "source": [
    "# Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "931b5408",
   "metadata": {},
   "source": [
    "The objective of the model is to detect a positive or negative sentiment giving a review. The steps in the data processing are oriented to clean the data before passing it to the model.\n",
    "\n",
    "Things done:\n",
    "- We have decided to only keep the text and the Score. \n",
    "- Since the possible ultime objective of the model is to detect mismatches in the review-sentiment we will only keep <= 2 Score as negative and >=4 as positive. The scores with 3 will b dropped\n",
    "- Cleaning the text so the vectorized version of the text is simpler\n",
    "\n",
    "In the cleaning, we have 2 pipelines: one for cleaning the sentences with a classes structure and another one with a function pipeline for those which uses tokens. This structure is dynamic to be able to easy iterate through different experiments.\n",
    "\n",
    "In the regex/sentences we have:\n",
    "- RemoveNumbers: remove all the words that contains numbers and numbers\n",
    "- RemoveHtml: remove html tags\n",
    "- RemoveUrl: remove all urls with http or www\n",
    "- RemovePatterns: remove all the words that has 3 or more of the same letters. Such as: aaaaa, zzzzzz or heeeelloooo\n",
    "- RemoveAbbreviations: replace abbreviations with their non-abbreviate form. E.g.: haven't -> have not\n",
    "\n",
    "In the tokens:\n",
    "- remove_stopwords: english base stopwords except from the abbreviated forms of the words.\n",
    "- stem_text: stemming\n",
    "- lemmatize_text: lematization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "393da04a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(568454, 10)\n"
     ]
    }
   ],
   "source": [
    "cleaned_data = pd.read_csv('cleaned_data.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8aea071f",
   "metadata": {},
   "source": [
    "## Binary labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1d3988f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_to_binary(cleaned_data):\n",
    "    df_binary = pd.DataFrame(cleaned_data, columns=['Score', 'Text'])\n",
    "    #create a binary dataset with only the positive or negative reviews\n",
    "\n",
    "    # Create a function to map scores to 0 or 1 based on your conditions\n",
    "    def label_score(score):\n",
    "        if int(score) >= 4:\n",
    "            return 1\n",
    "        elif int(score) <= 2:\n",
    "            return 0\n",
    "        else:\n",
    "            return None  # Ignore scores equal to 3\n",
    "\n",
    "    # Apply the function to the 'Score' column and create a new column 'Label'\n",
    "    df_binary['Label'] = df_binary['Score'].apply(label_score)\n",
    "\n",
    "    # Drop rows with Label equal to None (scores equal to 3)\n",
    "    df_binary = df_binary.dropna(subset=['Label'])\n",
    "\n",
    "    # Optionally, you can reset the index if you want\n",
    "    df_binary.reset_index(drop=True, inplace=True)\n",
    "    return df_binary\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c33cdf91",
   "metadata": {},
   "source": [
    "## Get only text and labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f2a3dfc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_binary = data_to_binary(cleaned_data)\n",
    "\n",
    "X = df_binary.Text\n",
    "y = df_binary.Label"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42e62d0a",
   "metadata": {},
   "source": [
    "## Text processing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcbbf193",
   "metadata": {},
   "source": [
    "### Functions for tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "13e0b2ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\a.ramirez.lopez\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\a.ramirez.lopez\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\a.ramirez.lopez\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from spellchecker import SpellChecker\n",
    "# import string\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "\n",
    "def stem_text(tokens):\n",
    "    stemmer = SnowballStemmer('english')\n",
    "    try:\n",
    "        tokens = [stemmer.stem(word) for word in tokens]\n",
    "    except TypeError:\n",
    "        print(tokens)\n",
    "    return tokens\n",
    "\n",
    "def lemmatize_text(tokens):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    tokens = [lemmatizer.lemmatize(word) for word in tokens]\n",
    "    return tokens\n",
    "\n",
    "\n",
    "def remove_stopwords(tokens):\n",
    "    default_stopwords = set(stopwords.words('english'))\n",
    "    excluding = set(['against','not','don', \"don't\",'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\",\n",
    "             'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", \n",
    "             'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\",'shouldn', \"shouldn't\", 'wasn',\n",
    "             \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"])\n",
    " \n",
    "    custom_stopwords = default_stopwords - excluding\n",
    "\n",
    "    tokens = [token for token in tokens if token not in custom_stopwords]\n",
    "    tokens = filter(None, tokens)\n",
    "    return tokens\n",
    "\n",
    "\n",
    "def correct_spelling(tokens):\n",
    "    spell = SpellChecker()\n",
    "    tokens = [spell.correction(word) for word in tokens]\n",
    "    tokens = filter(None, tokens)\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e28c7e1",
   "metadata": {},
   "source": [
    "### Sentence classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "fd4b3898",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from nltk.tokenize.treebank import TreebankWordDetokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "3b7123d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from abc import ABC\n",
    "\n",
    "\n",
    "class SentenceDfCleaner(ABC):\n",
    "\n",
    "    def __init__(self):\n",
    "        self.pattern: str\n",
    "\n",
    "    def clean(self, df):\n",
    "        return df.str.replace(self.pattern, '', regex=True)\n",
    "\n",
    "\n",
    "class RemoveNumbers(SentenceDfCleaner):\n",
    "\n",
    "    def __init__(self):\n",
    "        self.pattern = re.compile(\"\\S*\\d\\S*\")\n",
    "\n",
    "\n",
    "class RemoveHtml(SentenceDfCleaner):\n",
    "\n",
    "    def __init__(self):\n",
    "        self.pattern = re.compile('<.*?>')\n",
    "\n",
    "\n",
    "class RemoveUrl(SentenceDfCleaner):\n",
    "\n",
    "    def __init__(self):\n",
    "        self.pattern = re.compile('http\\S+|www.\\S+')\n",
    "\n",
    "\n",
    "class RemovePunctuations(SentenceDfCleaner):\n",
    "\n",
    "    def __init__(self):\n",
    "        self.pattern = re.compile('[^\\w\\s]')\n",
    "\n",
    "\n",
    "class RemovePatterns(SentenceDfCleaner):\n",
    "    \"\"\"\n",
    "    https://stackoverflow.com/questions/37012948/regex-to-match-an-entire-word-that-contains-repeated-character\n",
    "    Remove words like 'zzzzzzzzzzzzzzzzzzzzzzz', 'testtting', 'grrrrrrreeeettttt' etc. \n",
    "    Preserves words like 'looks', 'goods', 'soon' etc. We will remove all such words \n",
    "    which has three consecutive repeating characters.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        self.pattern = re.compile('\\\\s*\\\\b(?=\\\\w*(\\\\w)\\\\1{2,})\\\\w*\\\\b')\n",
    "\n",
    "\n",
    "class RemoveAbbreviations(SentenceDfCleaner):\n",
    "\n",
    "    def __init__(self):\n",
    "        self.abbr_dict = {\n",
    "            \"what's\":\"what is\",\n",
    "            \"what're\":\"what are\",\n",
    "            \"who's\":\"who is\",\n",
    "            \"who're\":\"who are\",\n",
    "            \"where's\":\"where is\",\n",
    "            \"where're\":\"where are\",\n",
    "            \"when's\":\"when is\",\n",
    "            \"when're\":\"when are\",\n",
    "            \"how's\":\"how is\",\n",
    "            \"how're\":\"how are\",\n",
    "\n",
    "            \"i'm\":\"i am\",\n",
    "            \"we're\":\"we are\",\n",
    "            \"you're\":\"you are\",\n",
    "            \"they're\":\"they are\",\n",
    "            \"it's\":\"it is\",\n",
    "            \"he's\":\"he is\",\n",
    "            \"she's\":\"she is\",\n",
    "            \"that's\":\"that is\",\n",
    "            \"there's\":\"there is\",\n",
    "            \"there're\":\"there are\",\n",
    "\n",
    "            \"i've\":\"i have\",\n",
    "            \"we've\":\"we have\",\n",
    "            \"you've\":\"you have\",\n",
    "            \"they've\":\"they have\",\n",
    "            \"who've\":\"who have\",\n",
    "            \"would've\":\"would have\",\n",
    "            \"not've\":\"not have\",\n",
    "\n",
    "            \"i'll\":\"i will\",\n",
    "            \"we'll\":\"we will\",\n",
    "            \"you'll\":\"you will\",\n",
    "            \"he'll\":\"he will\",\n",
    "            \"she'll\":\"she will\",\n",
    "            \"it'll\":\"it will\",\n",
    "            \"they'll\":\"they will\",\n",
    "\n",
    "            \"isn't\":\"is not\",\n",
    "            \"wasn't\":\"was not\",\n",
    "            \"aren't\":\"are not\",\n",
    "            \"weren't\":\"were not\",\n",
    "            \"can't\":\"can not\",\n",
    "            \"couldn't\":\"could not\",\n",
    "            \"don't\":\"do not\",\n",
    "            \"didn't\":\"did not\",\n",
    "            \"shouldn't\":\"should not\",\n",
    "            \"wouldn't\":\"would not\",\n",
    "            \"doesn't\":\"does not\",\n",
    "            \"haven't\":\"have not\",\n",
    "            \"hasn't\":\"has not\",\n",
    "            \"hadn't\":\"had not\",\n",
    "            \"won't\":\"will not\",\n",
    "            '\\s+':' '\n",
    "        }\n",
    "        self.pattern = re.compile(\"|\".join(map(re.escape, self.abbr_dict.keys())))\n",
    "    \n",
    "    def clean(self, df):\n",
    "        return df.str.replace(self.pattern, \n",
    "                              lambda match: self.abbr_dict[match.group(0)],\n",
    "                                regex=True)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cd460d7",
   "metadata": {},
   "source": [
    "### Pipeline functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "bee412dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ind_preprocess_text(text, processing_steps, tokenized=False):\n",
    "    ''' Put everything in lowercase, remove punctuation and stopwords --> possibility to do stemming or lemmatizaion'''\n",
    "    # Tokenize the text and convert to lowercase every word\n",
    "    if not isinstance(text, list):\n",
    "        tokens = word_tokenize(text)\n",
    "    else:\n",
    "        tokens = text\n",
    "    \n",
    "    for processing_step in processing_steps:\n",
    "        tokens = processing_step(tokens)\n",
    "    \n",
    "    if tokenized:\n",
    "        return tokens\n",
    "    # Join tokens back into a single string\n",
    "    return TreebankWordDetokenizer().detokenize(tokens)\n",
    "\n",
    "\n",
    "def preprocess_text(text_df, processing_steps, tokenized):\n",
    "    text_df = text_df.str.lower()\n",
    "\n",
    "    for sent_step in processing_steps['sentence']:\n",
    "        text_df = sent_step.clean(text_df)\n",
    "    \n",
    "    text_df = text_df.apply(ind_preprocess_text, \n",
    "                         processing_steps=processing_steps['tokens'], \n",
    "                         tokenized=tokenized)\n",
    "    return text_df\n",
    "\n",
    "\n",
    "processing_steps = {'sentence': [RemoveNumbers(), RemoveHtml(), RemoveUrl(), RemovePunctuations(), \n",
    "                                 RemovePatterns(), RemoveAbbreviations()],\n",
    "                    'tokens': [remove_stopwords, stem_text, lemmatize_text]}\n",
    "\n",
    "# Example usage:\n",
    "X_processed = preprocess_text(X, processing_steps, tokenized=False)\n",
    "# X_processed2 = preprocess_text(X_processed, processing_steps, tokenized=False, is_clean=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "54277086",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>Labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>bought sever vital can dog food product found ...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>product arriv label jumbo salt peanutsth peanu...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>confect around centuri light pillowi citrus ge...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>great taffi great price wide assort yummi taff...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>got wild hair taffi order five pound bag taffi...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Text  Labels\n",
       "0  bought sever vital can dog food product found ...     1.0\n",
       "1  product arriv label jumbo salt peanutsth peanu...     0.0\n",
       "2  confect around centuri light pillowi citrus ge...     1.0\n",
       "3  great taffi great price wide assort yummi taff...     1.0\n",
       "4  got wild hair taffi order five pound bag taffi...     1.0"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_processed = pd.concat([X_processed, y], axis=1)\n",
    "data_processed.columns = ['Text', 'Labels']\n",
    "data_processed.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "818f9e88",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_processed.to_csv('processed_text_with_all.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
