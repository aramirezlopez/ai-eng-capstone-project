{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import f1_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we will going to choose the techniques we are going to apply for the final pipeline. Since it is a first approach model, we decided to pick which model we are going to train in the following steps by cross-validating them with a very simple default LogisticRegression. \n",
    "- If one of the techniques are proven to be significantly better than the other, we will use that in the model pipeline \n",
    "- If not, the simplest one will be chosen."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We kept track of the data indexes, since some of the data will be a sparse numpy array. \n",
    "Even though the split was done by a seed, because we cannot ensure the consistency between library versions or OS systems."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have chosen the f1_macro metric, since it's a symmetric metric of f1 and we care the same about false positives and false negatives and getting right both of the categories. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('processed_text_with_all.csv')\n",
    "df = df.dropna()\n",
    "\n",
    "train_index = pd.read_csv('train_index.csv').values[:, 0]\n",
    "test_index = pd.read_csv('test_index.csv').values[:, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = df.Text.loc[train_index]\n",
    "X_test = df.Text.loc[test_index]\n",
    "y_train = df.Labels.loc[train_index]\n",
    "y_test = df.Labels.loc[test_index]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text to vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Algorithms to convert text into vector after having processing the text. We are going to try: CountVectorizer, TF-IDF and Word2Vec. The Word2Vec training is on the Word2VecTraining notebook, here we will only load the data.\n",
    "\n",
    "- We have chosen 300 as the number of features to avoid memory problems and overfitting, keeping a reasonable rate between the number of rows and columns. Then, we will compare all the models having their first 300 features.\n",
    "\n",
    "- We are going to train a 1 to 1-3 ngrams in the CountVectorizer and the TF-IDF model. The 2 ngram model to add the words with not and some compose words and the 3 to avoid left any useful extra information left."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_vect3 = CountVectorizer(ngram_range=(1, 3), max_features=300)\n",
    "X_train_cv3 = count_vect3.fit_transform(X_train) \n",
    "# X_val_cv = count_vect.transform(X_val)\n",
    "X_test_cv3 = count_vect3.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_vect2 = CountVectorizer(ngram_range=(1, 2), max_features=300)\n",
    "X_train_cv2 = count_vect2.fit_transform(X_train) \n",
    "# X_val_cv = count_vect.transform(X_val)\n",
    "X_test_cv2 = count_vect2.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_vect1 = CountVectorizer(ngram_range=(1, 1), max_features=300)\n",
    "X_train_cv1 = count_vect1.fit_transform(X_train) \n",
    "# X_val_cv = count_vect.transform(X_val)\n",
    "X_test_cv1 = count_vect1.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_vect2.vocabulary_ == count_vect3.vocabulary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_vect2.vocabulary_ == count_vect1.vocabulary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf3 = TfidfVectorizer(ngram_range=(1, 3), max_features=300)\n",
    "X_train_tfidf3 = tfidf3.fit_transform(X_train)\n",
    "# X_val_tfidf = tfidf.transform(X_val)\n",
    "X_test_tfidf3 = tfidf3.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf2 = TfidfVectorizer(ngram_range=(1, 2), max_features=300)\n",
    "X_train_tfidf2 = tfidf2.fit_transform(X_train)\n",
    "# X_val_tfidf = tfidf.transform(X_val)\n",
    "X_test_tfidf2 = tfidf2.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf1 = TfidfVectorizer(ngram_range=(1, 1), max_features=300)\n",
    "X_train_tfidf1 = tfidf1.fit_transform(X_train)\n",
    "# X_val_tfidf = tfidf.transform(X_val)\n",
    "X_test_tfidf1 = tfidf1.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf3.vocabulary_ == tfidf2.vocabulary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf2.vocabulary_ == tfidf1.vocabulary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_w2v = pd.read_csv('w2v_window10_train_data.csv', index_col=0)\n",
    "X_test_w2v = pd.read_csv('w2v_window10_test_data.csv', index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_from_models = {'count_vec_ngram1': {'x': X_train_cv1,\n",
    "                                               'y': y_train},\n",
    "                          'tfidf_ngram1': {'x': X_train_tfidf1,\n",
    "                                           'y': y_train},\n",
    "                          'count_vec_ngram2': {'x': X_train_cv2,\n",
    "                                               'y': y_train},\n",
    "                          'tfidf_ngram2': {'x': X_train_tfidf2,\n",
    "                                           'y': y_train},\n",
    "                          'word2vec': {'x': X_train_w2v,\n",
    "                                       'y': y_train}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "\n",
    "def generate_cross_scoring_table_from_transf_data(train_data_dict, \n",
    "                                                  pred_model, scoring='f1_macro', cv=10):\n",
    "\n",
    "    all_scores = [cross_val_score(pred_model, train_data['x'], \n",
    "                                  train_data['y'], cv=cv, scoring=scoring)\n",
    "                  for train_data in train_data_dict.values()]\n",
    "\n",
    "    return pd.DataFrame(all_scores, index=train_data_dict.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count_vec_ngram1</th>\n",
       "      <td>0.730720</td>\n",
       "      <td>0.744704</td>\n",
       "      <td>0.740865</td>\n",
       "      <td>0.744642</td>\n",
       "      <td>0.733056</td>\n",
       "      <td>0.746572</td>\n",
       "      <td>0.741397</td>\n",
       "      <td>0.748263</td>\n",
       "      <td>0.743674</td>\n",
       "      <td>0.738538</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tfidf_ngram1</th>\n",
       "      <td>0.747909</td>\n",
       "      <td>0.759543</td>\n",
       "      <td>0.754347</td>\n",
       "      <td>0.762461</td>\n",
       "      <td>0.754868</td>\n",
       "      <td>0.759464</td>\n",
       "      <td>0.753718</td>\n",
       "      <td>0.760397</td>\n",
       "      <td>0.758010</td>\n",
       "      <td>0.751561</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>count_vec_ngram2</th>\n",
       "      <td>0.734709</td>\n",
       "      <td>0.746985</td>\n",
       "      <td>0.743778</td>\n",
       "      <td>0.748821</td>\n",
       "      <td>0.737387</td>\n",
       "      <td>0.747985</td>\n",
       "      <td>0.744234</td>\n",
       "      <td>0.749215</td>\n",
       "      <td>0.743730</td>\n",
       "      <td>0.741885</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tfidf_ngram2</th>\n",
       "      <td>0.751512</td>\n",
       "      <td>0.761197</td>\n",
       "      <td>0.756324</td>\n",
       "      <td>0.765519</td>\n",
       "      <td>0.757312</td>\n",
       "      <td>0.760695</td>\n",
       "      <td>0.753588</td>\n",
       "      <td>0.762479</td>\n",
       "      <td>0.759185</td>\n",
       "      <td>0.753557</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>word2vec</th>\n",
       "      <td>0.819960</td>\n",
       "      <td>0.833779</td>\n",
       "      <td>0.826369</td>\n",
       "      <td>0.834525</td>\n",
       "      <td>0.830219</td>\n",
       "      <td>0.830770</td>\n",
       "      <td>0.827782</td>\n",
       "      <td>0.834607</td>\n",
       "      <td>0.831262</td>\n",
       "      <td>0.830440</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         0         1         2         3         4         5  \\\n",
       "count_vec_ngram1  0.730720  0.744704  0.740865  0.744642  0.733056  0.746572   \n",
       "tfidf_ngram1      0.747909  0.759543  0.754347  0.762461  0.754868  0.759464   \n",
       "count_vec_ngram2  0.734709  0.746985  0.743778  0.748821  0.737387  0.747985   \n",
       "tfidf_ngram2      0.751512  0.761197  0.756324  0.765519  0.757312  0.760695   \n",
       "word2vec          0.819960  0.833779  0.826369  0.834525  0.830219  0.830770   \n",
       "\n",
       "                         6         7         8         9  \n",
       "count_vec_ngram1  0.741397  0.748263  0.743674  0.738538  \n",
       "tfidf_ngram1      0.753718  0.760397  0.758010  0.751561  \n",
       "count_vec_ngram2  0.744234  0.749215  0.743730  0.741885  \n",
       "tfidf_ngram2      0.753588  0.762479  0.759185  0.753557  \n",
       "word2vec          0.827782  0.834607  0.831262  0.830440  "
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr = LogisticRegression()\n",
    "\n",
    "\n",
    "result = generate_cross_scoring_table_from_transf_data(train_data_from_models, \n",
    "                                                       lr, scoring='f1_macro', cv=10)\n",
    "\n",
    "\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The word2vec is clearly better than the rest, so we will keep the data and the model from the word2vec for the following steps."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Handle imbalance labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will be trying a simple random undersampling, random oversampling and SMOTE and see if the results of the model are better applying those techniques."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we decided not to use the same cross validation technique, to be able to evaluate the model in data that has not been sampling to avoid bias in the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from imblearn.over_sampling import SMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_us_res, y_us_res = RandomUnderSampler().fit_resample(X_train_w2v, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_os_res, y_os_res = RandomOverSampler().fit_resample(X_train_w2v, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_sm_res, y_sm_res = SMOTE().fit_resample(X_train_w2v, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "imb_models = {'base': None,\n",
    "              'undersampling': RandomUnderSampler(),\n",
    "              'oversampling': RandomOverSampler(),\n",
    "              'smote': SMOTE()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import ShuffleSplit\n",
    "\n",
    "\n",
    "def generate_imb_results_table(pred_model_class, imb_models, X_train_orig, y_train_orig):\n",
    "    all_results = []\n",
    "    for model in imb_models.values():\n",
    "        ss = ShuffleSplit(n_splits=10, test_size=0.2, random_state=0)\n",
    "        ind_results = []\n",
    "        for train_index, test_index in ss.split(X_train_w2v.values):\n",
    "            try:\n",
    "                x_train = X_train_orig.iloc[train_index]\n",
    "                y_train = y_train_orig.iloc[train_index]\n",
    "                x_test = X_train_orig.iloc[test_index]\n",
    "                y_test = y_train_orig.iloc[test_index]\n",
    "            except:\n",
    "                return train_index, test_index\n",
    "\n",
    "            if model:\n",
    "                x_train_imb, y_train_imb = model.fit_resample(x_train, y_train)\n",
    "            else:\n",
    "                x_train_imb, y_train_imb = x_train, y_train\n",
    "            pred_model = pred_model_class()\n",
    "            pred_model.fit(x_train_imb, y_train_imb)\n",
    "            y_pred = pred_model.predict(x_test)\n",
    "            ind_results.append(f1_score(y_test, y_pred, average='macro'))\n",
    "            \n",
    "        all_results.append(ind_results)\n",
    "    return pd.DataFrame(all_results, index=imb_models.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "imb_result = generate_imb_results_table(LogisticRegression, imb_models, X_train_w2v, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>base</th>\n",
       "      <td>0.830428</td>\n",
       "      <td>0.831779</td>\n",
       "      <td>0.830530</td>\n",
       "      <td>0.832414</td>\n",
       "      <td>0.826815</td>\n",
       "      <td>0.827950</td>\n",
       "      <td>0.828817</td>\n",
       "      <td>0.826058</td>\n",
       "      <td>0.829886</td>\n",
       "      <td>0.828536</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>undersampling</th>\n",
       "      <td>0.804243</td>\n",
       "      <td>0.807803</td>\n",
       "      <td>0.801824</td>\n",
       "      <td>0.802430</td>\n",
       "      <td>0.803316</td>\n",
       "      <td>0.801522</td>\n",
       "      <td>0.803760</td>\n",
       "      <td>0.802825</td>\n",
       "      <td>0.803799</td>\n",
       "      <td>0.803864</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>oversampling</th>\n",
       "      <td>0.804965</td>\n",
       "      <td>0.808338</td>\n",
       "      <td>0.803273</td>\n",
       "      <td>0.802389</td>\n",
       "      <td>0.803654</td>\n",
       "      <td>0.803031</td>\n",
       "      <td>0.805854</td>\n",
       "      <td>0.803048</td>\n",
       "      <td>0.804639</td>\n",
       "      <td>0.804271</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>smote</th>\n",
       "      <td>0.810347</td>\n",
       "      <td>0.814086</td>\n",
       "      <td>0.809512</td>\n",
       "      <td>0.807804</td>\n",
       "      <td>0.808141</td>\n",
       "      <td>0.808434</td>\n",
       "      <td>0.809850</td>\n",
       "      <td>0.808486</td>\n",
       "      <td>0.809418</td>\n",
       "      <td>0.808970</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      0         1         2         3         4         5  \\\n",
       "base           0.830428  0.831779  0.830530  0.832414  0.826815  0.827950   \n",
       "undersampling  0.804243  0.807803  0.801824  0.802430  0.803316  0.801522   \n",
       "oversampling   0.804965  0.808338  0.803273  0.802389  0.803654  0.803031   \n",
       "smote          0.810347  0.814086  0.809512  0.807804  0.808141  0.808434   \n",
       "\n",
       "                      6         7         8         9  \n",
       "base           0.828817  0.826058  0.829886  0.828536  \n",
       "undersampling  0.803760  0.802825  0.803799  0.803864  \n",
       "oversampling   0.805854  0.803048  0.804639  0.804271  \n",
       "smote          0.809850  0.808486  0.809418  0.808970  "
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imb_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import ttest_ind, wilcoxon\n",
    "\n",
    "\n",
    "def test_ditr_significantly_greater_than_another(data1, data2, test_type, alpha=0.05):\n",
    "\n",
    "    print(f'----------------------- {test_type.capitalize()} -----------------------')\n",
    "\n",
    "    if test_type == 'ttest':\n",
    "        test_func = ttest_ind\n",
    "    elif test_type == 'wilcoxon':\n",
    "        test_func = wilcoxon\n",
    "    else:\n",
    "        raise ValueError(f'test_type must be ttest or wilcoxon not {test_type}')\n",
    "\n",
    "    statistic, p_value = test_func(data1, data2, alternative='greater')\n",
    "    \n",
    "    if p_value < alpha:\n",
    "        print(\"The mean of the first distribution is significantly\" +\n",
    "            \" greater than the mean of the second one.\")\n",
    "    else:\n",
    "        print(\"The mean of the first distribution is not significantly\" +\n",
    "            \" greater than the mean of the second one.\")\n",
    "        \n",
    "    return statistic, p_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------- Ttest -----------------------\n",
      "The mean of the first distribution is significantly greater than the mean of the second one.\n",
      "----------------------- Wilcoxon -----------------------\n",
      "The mean of the first distribution is significantly greater than the mean of the second one.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(55.0, 0.0009765625)"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "statistic, p_value = ttest_ind(imb_result.loc['base'],\n",
    "                               imb_result.loc['oversampling'], \n",
    "                               alternative='greater')\n",
    "\n",
    "test_ditr_significantly_greater_than_another(imb_result.loc['base'], \n",
    "                                             imb_result.loc['oversampling'], \n",
    "                                             'ttest')\n",
    "    \n",
    "test_ditr_significantly_greater_than_another(imb_result.loc['base'], \n",
    "                                             imb_result.loc['oversampling'], \n",
    "                                             'wilcoxon')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------- Ttest -----------------------\n",
      "The mean of the first distribution is significantly greater than the mean of the second one.\n",
      "----------------------- Wilcoxon -----------------------\n",
      "The mean of the first distribution is significantly greater than the mean of the second one.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(55.0, 0.0009765625)"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_ditr_significantly_greater_than_another(imb_result.loc['base'], \n",
    "                                             imb_result.loc['undersampling'], \n",
    "                                             'ttest')\n",
    "    \n",
    "test_ditr_significantly_greater_than_another(imb_result.loc['base'], \n",
    "                                             imb_result.loc['undersampling'], \n",
    "                                             'wilcoxon')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------- Ttest -----------------------\n",
      "The mean of the first distribution is significantly greater than the mean of the second one.\n",
      "----------------------- Wilcoxon -----------------------\n",
      "The mean of the first distribution is significantly greater than the mean of the second one.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(55.0, 0.0009765625)"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_ditr_significantly_greater_than_another(imb_result.loc['base'], \n",
    "                                             imb_result.loc['smote'], \n",
    "                                             'ttest')\n",
    "    \n",
    "test_ditr_significantly_greater_than_another(imb_result.loc['base'], \n",
    "                                             imb_result.loc['smote'], \n",
    "                                             'wilcoxon')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems like for this dataset, the imbalancing techniques worsen the results. Therefore, we are skipping them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "academy",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
