{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Training Deployment"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Getting the data and connecting to Azure"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 68,
      "metadata": {
        "attributes": {
          "classes": [
            "Python"
          ],
          "id": ""
        },
        "gather": {
          "logged": 1709650467689
        },
        "name": "import-mlclient"
      },
      "outputs": [],
      "source": [
        "# Handle to the workspace\n",
        "from azure.ai.ml import MLClient\n",
        "\n",
        "# Authentication package\n",
        "from azure.identity import DefaultAzureCredential, InteractiveBrowserCredential\n",
        "\n",
        "try:\n",
        "    credential = DefaultAzureCredential()\n",
        "    # Check if given credential can get token successfully.\n",
        "    credential.get_token(\"https://management.azure.com/.default\")\n",
        "except Exception as ex:\n",
        "    # Fall back to InteractiveBrowserCredential in case DefaultAzureCredential not work\n",
        "    credential = InteractiveBrowserCredential()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 69,
      "metadata": {
        "attributes": {
          "classes": [
            "Python"
          ],
          "id": ""
        },
        "gather": {
          "logged": 1709650468415
        },
        "name": "ml_client"
      },
      "outputs": [],
      "source": [
        "# Get a handle to the workspace\n",
        "ml_client = MLClient(\n",
        "    credential=credential,\n",
        "    subscription_id=\"32787e59-b8b4-4923-89db-73cff7a82fbf\",\n",
        "    resource_group_name=\"capstone-project\",\n",
        "    workspace_name=\"capstone-project-workspace\",\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Getting the data asset of the cleaned data for training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 70,
      "metadata": {
        "gather": {
          "logged": 1709650469228
        },
        "name": "credit_data"
      },
      "outputs": [],
      "source": [
        "from azure.ai.ml.entities import Data\n",
        "from azure.ai.ml.constants import AssetTypes\n",
        "\n",
        "my_path = \"azureml://subscriptions/32787e59-b8b4-4923-89db-73cff7a82fbf/resourcegroups/capstone-project/workspaces/capstone-project-workspace/datastores/workspaceblobstore/paths/UI/2024-03-05_102332_UTC/cleaned_data.csv\"\n",
        "\n",
        "my_data = Data(\n",
        "    path=my_path,\n",
        "    type=AssetTypes.URI_FILE,\n",
        "    description=\"this is my first data asset\",\n",
        "    name=\"dataset1\",\n",
        "    version=\"1\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 71,
      "metadata": {
        "attributes": {
          "classes": [
            "Python"
          ],
          "id": ""
        },
        "gather": {
          "logged": 1709650471566
        },
        "name": "update-credit_data"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Dataset with name dataset1 was registered to workspace, the dataset version is 1\n"
          ]
        }
      ],
      "source": [
        "my_data = ml_client.data.create_or_update(my_data)\n",
        "print(\n",
        "    f\"Dataset with name {my_data.name} was registered to workspace, the dataset version is {my_data.version}\"\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Create a job environment for pipeline steps"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 72,
      "metadata": {
        "gather": {
          "logged": 1709650472300
        },
        "name": "dependencies_dir"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "dependencies_dir = \"./dependencies\"\n",
        "os.makedirs(dependencies_dir, exist_ok=True)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We added the nltk, gensim and imblearn libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 73,
      "metadata": {
        "attributes": {
          "classes": [
            "Python"
          ],
          "id": ""
        },
        "name": "conda.yaml"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Overwriting ./dependencies/conda.yaml\n"
          ]
        }
      ],
      "source": [
        "%%writefile {dependencies_dir}/conda.yaml\n",
        "name: model-env\n",
        "channels:\n",
        "  - conda-forge\n",
        "dependencies:\n",
        "  - python=3.8\n",
        "  - numpy=1.21.2\n",
        "  - pip=21.2.4\n",
        "  - scikit-learn=0.24.2\n",
        "  - scipy=1.7.1\n",
        "  - pandas>=1.1,<1.2\n",
        "  - nltk=3.8.1\n",
        "  - pip:\n",
        "    - inference-schema[numpy-support]==1.3.0\n",
        "    - xlrd==2.0.1\n",
        "    - azureml-mlflow==1.42.0\n",
        "    - gensim==4.3.2\n",
        "    - imblearn==0.12.0"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The specification contains some usual packages, that you'll use in your pipeline (numpy, pip).\n",
        "\n",
        "\n",
        "Use the *yaml* file to create and register this custom environment in your workspace:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 97,
      "metadata": {
        "attributes": {
          "classes": [
            "Python"
          ],
          "id": ""
        },
        "gather": {
          "logged": 1709651513869
        },
        "name": "custom_env_name"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Environment with name aml-scikit-learn is registered to workspace, the environment version is 0.1.2\n"
          ]
        }
      ],
      "source": [
        "from azure.ai.ml.entities import Environment\n",
        "\n",
        "custom_env_name = \"aml-scikit-learn\"\n",
        "\n",
        "pipeline_job_env = Environment(\n",
        "    name=custom_env_name,\n",
        "    description=\"Custom environment for Capstone Project\",\n",
        "    tags={\"scikit-learn\": \"0.24.2\"},\n",
        "    conda_file=os.path.join(dependencies_dir, \"conda.yaml\"),\n",
        "    image=\"mcr.microsoft.com/azureml/openmpi4.1.0-ubuntu20.04:latest\",\n",
        "    version=\"0.1.2\",\n",
        ")\n",
        "pipeline_job_env = ml_client.environments.create_or_update(pipeline_job_env)\n",
        "\n",
        "print(\n",
        "    f\"Environment with name {pipeline_job_env.name} is registered to workspace, the environment version is {pipeline_job_env.version}\"\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Build the training pipeline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Create data prep component"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 98,
      "metadata": {
        "attributes": {
          "classes": [
            "Python"
          ],
          "id": ""
        },
        "gather": {
          "logged": 1709651514007
        },
        "name": "data_prep_src_dir"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "data_prep_src_dir = \"./components/data_prep\"\n",
        "os.makedirs(data_prep_src_dir, exist_ok=True)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This script performs the simple task of splitting the data into train and test datasets. \n",
        "\n",
        "[MLFlow](https://mlflow.org/docs/latest/tracking.html) will be used to log the parameters and metrics during our pipeline run."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 99,
      "metadata": {
        "attributes": {
          "classes": [
            "Python"
          ],
          "id": ""
        },
        "name": "def-main"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Overwriting ./components/data_prep/data_prep.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile {data_prep_src_dir}/data_prep.py\n",
        "import os\n",
        "import argparse\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "import logging\n",
        "import mlflow\n",
        "\n",
        "\n",
        "def data_to_binary(cleaned_data):\n",
        "    df_binary = pd.DataFrame(cleaned_data, columns=['Score', 'Text'])\n",
        "    #create a binary dataset with only the positive or negative reviews\n",
        "\n",
        "    # Create a function to map scores to 0 or 1 based on your conditions\n",
        "    def label_score(score):\n",
        "        if int(score) >= 4:\n",
        "            return 1\n",
        "        elif int(score) <= 2:\n",
        "            return 0\n",
        "        else:\n",
        "            return None  # Ignore scores equal to 3\n",
        "\n",
        "    # Apply the function to the 'Score' column and create a new column 'Label'\n",
        "    df_binary['Label'] = df_binary['Score'].apply(label_score)\n",
        "\n",
        "    # Drop rows with Label equal to None (scores equal to 3)\n",
        "    df_binary = df_binary.dropna(subset=['Label'])\n",
        "\n",
        "    # Optionally, you can reset the index if you want\n",
        "    df_binary.reset_index(drop=True, inplace=True)\n",
        "    return df_binary\n",
        "\n",
        "\n",
        "\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "import string\n",
        "from nltk.stem.snowball import SnowballStemmer\n",
        "import re\n",
        "\n",
        "\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "\n",
        "def stem_text(tokens):\n",
        "    stemmer = SnowballStemmer('english')\n",
        "    try:\n",
        "        tokens = [stemmer.stem(word) for word in tokens]\n",
        "    except TypeError:\n",
        "        print(tokens)\n",
        "    return tokens\n",
        "\n",
        "def lemmatize_text(tokens):\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    tokens = [lemmatizer.lemmatize(word) for word in tokens]\n",
        "    return tokens\n",
        "\n",
        "\n",
        "def remove_punctuation(tokens):\n",
        "    tokens = [token for token in tokens if token not in string.punctuation]\n",
        "    return tokens\n",
        "\n",
        "\n",
        "def remove_stopwords(tokens):\n",
        "    default_stopwords = set(stopwords.words('english'))\n",
        "    excluding = set(['against','not','don', \"don't\",'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\",\n",
        "             'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", \n",
        "             'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\",'shouldn', \"shouldn't\", 'wasn',\n",
        "             \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"])\n",
        " \n",
        "    custom_stopwords = default_stopwords - excluding\n",
        "\n",
        "    tokens = [token for token in tokens if token not in custom_stopwords]\n",
        "    tokens = filter(None, tokens)\n",
        "    return tokens\n",
        "\n",
        "\n",
        "def correct_spelling_words_more_same_three_letters(tokens):\n",
        "    tokens = [token for token in tokens if not any(char * 3 in token for char in string.ascii_lowercase)]\n",
        "    return tokens\n",
        "\n",
        "\n",
        "from abc import ABC\n",
        "\n",
        "\n",
        "class SentenceDfCleaner(ABC):\n",
        "\n",
        "    def __init__(self):\n",
        "        self.pattern: str\n",
        "\n",
        "    def clean(self, df):\n",
        "        return df.str.replace(self.pattern, '', regex=True)\n",
        "\n",
        "\n",
        "class RemoveNumbers(SentenceDfCleaner):\n",
        "\n",
        "    def __init__(self):\n",
        "        self.pattern = re.compile(\"\\S*\\d\\S*\")\n",
        "\n",
        "\n",
        "class RemoveHtml(SentenceDfCleaner):\n",
        "\n",
        "    def __init__(self):\n",
        "        self.pattern = re.compile('<.*?>')\n",
        "\n",
        "\n",
        "class RemoveUrl(SentenceDfCleaner):\n",
        "\n",
        "    def __init__(self):\n",
        "        self.pattern = re.compile('http\\S+|www.\\S+')\n",
        "\n",
        "\n",
        "class RemovePunctuations(SentenceDfCleaner):\n",
        "\n",
        "    def __init__(self):\n",
        "        self.pattern = re.compile('[^\\w\\s]')\n",
        "\n",
        "\n",
        "class RemovePatterns(SentenceDfCleaner):\n",
        "    \"\"\"\n",
        "    https://stackoverflow.com/questions/37012948/regex-to-match-an-entire-word-that-contains-repeated-character\n",
        "    Remove words like 'zzzzzzzzzzzzzzzzzzzzzzz', 'testtting', 'grrrrrrreeeettttt' etc. \n",
        "    Preserves words like 'looks', 'goods', 'soon' etc. We will remove all such words \n",
        "    which has three consecutive repeating characters.\n",
        "    \"\"\"\n",
        "    def __init__(self):\n",
        "        self.pattern = re.compile('\\\\s*\\\\b(?=\\\\w*(\\\\w)\\\\1{2,})\\\\w*\\\\b')\n",
        "\n",
        "\n",
        "class RemoveAbbreviations(SentenceDfCleaner):\n",
        "\n",
        "    def __init__(self):\n",
        "        self.abbr_dict = {\n",
        "            \"what's\":\"what is\",\n",
        "            \"what're\":\"what are\",\n",
        "            \"who's\":\"who is\",\n",
        "            \"who're\":\"who are\",\n",
        "            \"where's\":\"where is\",\n",
        "            \"where're\":\"where are\",\n",
        "            \"when's\":\"when is\",\n",
        "            \"when're\":\"when are\",\n",
        "            \"how's\":\"how is\",\n",
        "            \"how're\":\"how are\",\n",
        "\n",
        "            \"i'm\":\"i am\",\n",
        "            \"we're\":\"we are\",\n",
        "            \"you're\":\"you are\",\n",
        "            \"they're\":\"they are\",\n",
        "            \"it's\":\"it is\",\n",
        "            \"he's\":\"he is\",\n",
        "            \"she's\":\"she is\",\n",
        "            \"that's\":\"that is\",\n",
        "            \"there's\":\"there is\",\n",
        "            \"there're\":\"there are\",\n",
        "\n",
        "            \"i've\":\"i have\",\n",
        "            \"we've\":\"we have\",\n",
        "            \"you've\":\"you have\",\n",
        "            \"they've\":\"they have\",\n",
        "            \"who've\":\"who have\",\n",
        "            \"would've\":\"would have\",\n",
        "            \"not've\":\"not have\",\n",
        "\n",
        "            \"i'll\":\"i will\",\n",
        "            \"we'll\":\"we will\",\n",
        "            \"you'll\":\"you will\",\n",
        "            \"he'll\":\"he will\",\n",
        "            \"she'll\":\"she will\",\n",
        "            \"it'll\":\"it will\",\n",
        "            \"they'll\":\"they will\",\n",
        "\n",
        "            \"isn't\":\"is not\",\n",
        "            \"wasn't\":\"was not\",\n",
        "            \"aren't\":\"are not\",\n",
        "            \"weren't\":\"were not\",\n",
        "            \"can't\":\"can not\",\n",
        "            \"couldn't\":\"could not\",\n",
        "            \"don't\":\"do not\",\n",
        "            \"didn't\":\"did not\",\n",
        "            \"shouldn't\":\"should not\",\n",
        "            \"wouldn't\":\"would not\",\n",
        "            \"doesn't\":\"does not\",\n",
        "            \"haven't\":\"have not\",\n",
        "            \"hasn't\":\"has not\",\n",
        "            \"hadn't\":\"had not\",\n",
        "            \"won't\":\"will not\",\n",
        "            '\\s+':' '\n",
        "        }\n",
        "        self.pattern = re.compile(\"|\".join(map(re.escape, self.abbr_dict.keys())))\n",
        "    \n",
        "    def clean(self, df):\n",
        "        return df.str.replace(self.pattern, \n",
        "                              lambda match: self.abbr_dict[match.group(0)],\n",
        "                                regex=True)\n",
        "\n",
        "\n",
        "def ind_preprocess_text(text, processing_steps, tokenized=False):\n",
        "    ''' Put everything in lowercase, remove punctuation and stopwords --> possibility to do stemming or lemmatizaion'''\n",
        "    # Tokenize the text and convert to lowercase every word\n",
        "    if not isinstance(text, list):\n",
        "        tokens = word_tokenize(text)\n",
        "    else:\n",
        "        tokens = text\n",
        "    \n",
        "    for processing_step in processing_steps:\n",
        "        tokens = processing_step(tokens)\n",
        "    \n",
        "    if tokenized:\n",
        "        return tokens\n",
        "    # Join tokens back into a single string\n",
        "    return TreebankWordDetokenizer().detokenize(tokens)\n",
        "\n",
        "\n",
        "def preprocess_text(text_df, processing_steps, tokenized):\n",
        "    text_df = text_df.str.lower()\n",
        "\n",
        "    for sent_step in processing_steps['sentence']:\n",
        "        text_df = sent_step.clean(text_df)\n",
        "    \n",
        "    text_df = text_df.apply(ind_preprocess_text, \n",
        "                         processing_steps=processing_steps['tokens'], \n",
        "                         tokenized=tokenized)\n",
        "    return text_df\n",
        "\n",
        "\n",
        "def main():\n",
        "    \"\"\"Main function of the script.\"\"\"\n",
        "\n",
        "    # input and output arguments\n",
        "    parser = argparse.ArgumentParser()\n",
        "    parser.add_argument(\"--data\", type=str, help=\"path to input data\")\n",
        "    parser.add_argument(\"--test_train_ratio\", type=float, required=False, default=0.25)\n",
        "    parser.add_argument(\"--train_data\", type=str, help=\"path to train data\")\n",
        "    parser.add_argument(\"--train_labels\", type=str, help=\"path to train labels\")\n",
        "    parser.add_argument(\"--test_data\", type=str, help=\"path to test data\")\n",
        "    parser.add_argument(\"--test_labels\", type=str, help=\"path to test labels\")\n",
        "    args = parser.parse_args()\n",
        "\n",
        "    print(args.train_data, args.train_labels, args.test_data, args.test_labels)\n",
        "\n",
        "    # Start Logging\n",
        "    mlflow.start_run()\n",
        "\n",
        "    print(\" \".join(f\"{k}={v}\" for k, v in vars(args).items()))\n",
        "\n",
        "    print(\"input data:\", args.data)\n",
        "\n",
        "    df = pd.read_csv(args.data).iloc[:100]\n",
        "    print(df.head())\n",
        "    print(df.columns)\n",
        "\n",
        "    mlflow.log_metric(\"num_samples\", df.shape[0])\n",
        "    mlflow.log_metric(\"num_features\", df.shape[1] - 1)\n",
        "\n",
        "    df_binary = data_to_binary(df)\n",
        "    mlflow.log_metric(\"binary_num_samples\", df.shape[0])\n",
        "    mlflow.log_metric(\"binary_num_features\", df.shape[1] - 1)\n",
        "\n",
        "    processing_steps = {'sentence': [RemoveNumbers(), RemoveHtml(), RemoveUrl(), RemovePunctuations(), \n",
        "                                    RemovePatterns(), RemoveAbbreviations()],\n",
        "                        'tokens': [remove_stopwords, stem_text, lemmatize_text]}\n",
        "\n",
        "    X_processed = preprocess_text(X, processing_steps, tokenized=False)\n",
        "    \n",
        "    X_train, X_test, y_train, y_test = train_test_split(X_processed, df_binary.Label, \n",
        "                                                        test_size=args.test_train_ratio,\n",
        "                                                        random_state=99)\n",
        "\n",
        "    # output paths are mounted as folder, therefore, we are adding a filename to the path\n",
        "    X_train.to_csv(os.path.join(args.train_data, \"data.csv\"), index=False)\n",
        "    y_train.to_csv(os.path.join(args.train_labels, \"data.csv\"), index=False)\n",
        "\n",
        "    X_test.to_csv(os.path.join(args.test_data, \"data.csv\"), index=False)\n",
        "    y_test.to_csv(os.path.join(args.test_labels, \"data.csv\"), index=False)\n",
        "\n",
        "    # Stop Logging\n",
        "    mlflow.end_run()\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 100,
      "metadata": {
        "gather": {
          "logged": 1709651514262
        },
        "name": "data_prep_component"
      },
      "outputs": [],
      "source": [
        "from azure.ai.ml import command\n",
        "from azure.ai.ml import Input, Output\n",
        "\n",
        "data_prep_component = command(\n",
        "    name=\"data_prep_credit_defaults\",\n",
        "    display_name=\"Data preparation for training\",\n",
        "    description=\"reads a .csv input, split the input to train and test\",\n",
        "    inputs={\n",
        "        \"data\": Input(type=\"uri_folder\"),\n",
        "        \"test_train_ratio\": Input(type=\"number\"),\n",
        "    },\n",
        "    outputs=dict(\n",
        "        train_data=Output(type=\"uri_folder\", mode=\"rw_mount\"),\n",
        "        test_data=Output(type=\"uri_folder\", mode=\"rw_mount\"),\n",
        "        train_labels=Output(type=\"uri_folder\", mode=\"rw_mount\"),\n",
        "        test_labels=Output(type=\"uri_folder\", mode=\"rw_mount\"),\n",
        "    ),\n",
        "    # The source folder of the component\n",
        "    code=data_prep_src_dir,\n",
        "    command=\"\"\"python data_prep.py \\\n",
        "            --data ${{inputs.data}} --test_train_ratio ${{inputs.test_train_ratio}} \\\n",
        "            --train_data ${{outputs.train_data}} --test_data ${{outputs.test_data}} \\\n",
        "            --train_labels ${{outputs.train_labels}} --test_labels ${{outputs.test_labels}} \\\n",
        "            \"\"\",\n",
        "    environment=f\"{pipeline_job_env.name}:{pipeline_job_env.version}\",\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 101,
      "metadata": {
        "gather": {
          "logged": 1709651517053
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Component data_prep_credit_defaults with Version 2024-03-05-15-11-54-5196075 is registered\n"
          ]
        }
      ],
      "source": [
        "# Now we register the component to the workspace\n",
        "data_prep_component = ml_client.create_or_update(data_prep_component.component)\n",
        "\n",
        "# Create (register) the component in your workspace\n",
        "print(\n",
        "    f\"Component {data_prep_component.name} with Version {data_prep_component.version} is registered\"\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Create training component"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 102,
      "metadata": {
        "attributes": {
          "classes": [
            "Python"
          ],
          "id": ""
        },
        "gather": {
          "logged": 1709651517237
        },
        "name": "train_src_dir"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "train_src_dir = \"./components/train\"\n",
        "os.makedirs(train_src_dir, exist_ok=True)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Create the training script in the directory:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 103,
      "metadata": {
        "attributes": {
          "classes": [
            "Python"
          ],
          "id": ""
        },
        "name": "train.py"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Overwriting ./components/train/train.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile {train_src_dir}/train.py\n",
        "import argparse\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import classification_report\n",
        "import os\n",
        "import pandas as pd\n",
        "import mlflow\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "\n",
        "def text_to_vec_transformation(X_train, X_test):\n",
        "    vect = TfidfVectorizer()\n",
        "    X_train_dtm = vect.fit_transform(X_train) \n",
        "\n",
        "    X_test_dtm = vect.transform(X_test)\n",
        "    return X_train_dtm, X_test_dtm\n",
        "\n",
        "\n",
        "def select_first_file(path):\n",
        "    \"\"\"Selects first file in folder, use under assumption there is only one file in folder\n",
        "    Args:\n",
        "        path (str): path to directory or file to choose\n",
        "    Returns:\n",
        "        str: full path of selected file\n",
        "    \"\"\"\n",
        "    files = os.listdir(path)\n",
        "    return os.path.join(path, files[0])\n",
        "\n",
        "\n",
        "# Start Logging\n",
        "mlflow.start_run()\n",
        "\n",
        "# enable autologging\n",
        "mlflow.sklearn.autolog()\n",
        "\n",
        "os.makedirs(\"./outputs\", exist_ok=True)\n",
        "\n",
        "\n",
        "def main():\n",
        "    \"\"\"Main function of the script.\"\"\"\n",
        "\n",
        "    # input and output arguments\n",
        "    parser = argparse.ArgumentParser()\n",
        "    parser.add_argument(\"--train_data\", type=str, help=\"path to train data\")\n",
        "    parser.add_argument(\"--test_data\", type=str, help=\"path to test data\")\n",
        "    parser.add_argument(\"--train_labels\", type=str, help=\"path to train labels\")\n",
        "    parser.add_argument(\"--test_labels\", type=str, help=\"path to test labels\")\n",
        "    parser.add_argument(\"--registered_model_name\", type=str, help=\"model name\")\n",
        "    parser.add_argument(\"--model\", type=str, help=\"path to model file\")\n",
        "    args = parser.parse_args()\n",
        "\n",
        "    print(args.train_data, args.train_labels, args.test_data, args.test_labels)\n",
        "\n",
        "    # paths are mounted as folder, therefore, we are selecting the file from folder\n",
        "    X_train = pd.read_csv(select_first_file(args.train_data)).values[:, 0]\n",
        "\n",
        "    # Extracting the label column\n",
        "    y_train = pd.read_csv(select_first_file(args.train_labels))\n",
        "\n",
        "    # paths are mounted as folder, therefore, we are selecting the file from folder\n",
        "    X_test = pd.read_csv(select_first_file(args.test_data)).values[:, 0]\n",
        "\n",
        "    # Extracting the label column\n",
        "    y_test = pd.read_csv(select_first_file(args.test_labels))\n",
        "\n",
        "    print(f\"Training with data of shape {X_train.shape}\")\n",
        "\n",
        "    X_train, X_test = text_to_vec_transformation(X_train, X_test)\n",
        "\n",
        "    model = LogisticRegression()\n",
        "    model.fit(X_train, y_train)\n",
        "\n",
        "    y_pred = model.predict(X_test)\n",
        "\n",
        "    print(classification_report(y_test, y_pred))\n",
        "\n",
        "    # Registering the model to the workspace\n",
        "    print(\"Registering the model via MLFlow\")\n",
        "    mlflow.sklearn.log_model(\n",
        "        sk_model=clf,\n",
        "        registered_model_name=args.registered_model_name,\n",
        "        artifact_path=args.registered_model_name,\n",
        "    )\n",
        "\n",
        "    # Saving the model to a file\n",
        "    mlflow.sklearn.save_model(\n",
        "        sk_model=clf,\n",
        "        path=os.path.join(args.model, \"trained_model\"),\n",
        "    )\n",
        "\n",
        "    # Stop Logging\n",
        "    mlflow.end_run()\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 104,
      "metadata": {
        "attributes": {
          "classes": [
            "Python"
          ],
          "id": ""
        },
        "name": "train.yml"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Overwriting ./components/train/train.yml\n"
          ]
        }
      ],
      "source": [
        "%%writefile {train_src_dir}/train.yml\n",
        "# <component>\n",
        "name: train_credit_defaults_model\n",
        "display_name: Train Credit Defaults Model\n",
        "# version: 1 # Not specifying a version will automatically update the version\n",
        "type: command\n",
        "inputs:\n",
        "  train_data: \n",
        "    type: uri_folder\n",
        "  test_data: \n",
        "    type: uri_folder\n",
        "  train_labels: \n",
        "    type: uri_folder\n",
        "  test_labels: \n",
        "    type: uri_folder  \n",
        "  registered_model_name:\n",
        "    type: string\n",
        "outputs:\n",
        "  model:\n",
        "    type: uri_folder\n",
        "code: .\n",
        "environment:\n",
        "  # for this step, we'll use an AzureML curate environment\n",
        "  azureml:AzureML-sklearn-1.0-ubuntu20.04-py38-cpu:1\n",
        "command: >-\n",
        "  python train.py \n",
        "  --train_data ${{inputs.train_data}} \n",
        "  --test_data ${{inputs.test_data}} \n",
        "  --train_labels ${{inputs.train_labels}} \n",
        "  --test_labels ${{inputs.test_labels}} \n",
        "  --registered_model_name ${{inputs.registered_model_name}} \n",
        "  --model ${{outputs.model}}\n",
        "# </component>\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Once the `yaml` file and the script are ready, you can create your component using `load_component()`. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 105,
      "metadata": {
        "attributes": {
          "classes": [
            "Python"
          ],
          "id": ""
        },
        "gather": {
          "logged": 1709651517765
        },
        "name": "train_component"
      },
      "outputs": [],
      "source": [
        "# importing the Component Package\n",
        "from azure.ai.ml import load_component\n",
        "\n",
        "# Loading the component from the yml file\n",
        "train_component = load_component(source=os.path.join(train_src_dir, \"train.yml\"))"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now create and register the component:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 106,
      "metadata": {
        "attributes": {
          "classes": [
            "Python"
          ],
          "id": ""
        },
        "gather": {
          "logged": 1709651519541
        },
        "name": "update-train_component"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r\u001b[32mUploading train (0.0 MBs):   0%|          | 0/4055 [00:00<?, ?it/s]\r\u001b[32mUploading train (0.0 MBs): 100%|██████████| 4055/4055 [00:00<00:00, 61199.76it/s]\n",
            "\u001b[39m\n",
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Component train_credit_defaults_model with Version 2024-03-05-15-11-58-1092337 is registered\n"
          ]
        }
      ],
      "source": [
        "# Now we register the component to the workspace\n",
        "train_component = ml_client.create_or_update(train_component)\n",
        "\n",
        "# Create (register) the component in your workspace\n",
        "print(\n",
        "    f\"Component {train_component.name} with Version {train_component.version} is registered\"\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Create the pipeline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 107,
      "metadata": {
        "attributes": {
          "classes": [
            "Python"
          ],
          "id": ""
        },
        "gather": {
          "logged": 1709651519699
        },
        "name": "pipeline"
      },
      "outputs": [],
      "source": [
        "# the dsl decorator tells the sdk that we are defining an Azure ML pipeline\n",
        "from azure.ai.ml import dsl, Input, Output\n",
        "\n",
        "\n",
        "@dsl.pipeline(\n",
        "    compute=\"serverless\",\n",
        "    description=\"E2E data_perp-train pipeline\",\n",
        ")\n",
        "def project_training_pipeline(\n",
        "    pipeline_job_data_input,\n",
        "    pipeline_job_test_train_ratio,\n",
        "    pipeline_job_registered_model_name,\n",
        "):\n",
        "    # using data_prep_function like a python call with its own inputs\n",
        "    data_prep_job = data_prep_component(\n",
        "        data=pipeline_job_data_input,\n",
        "        test_train_ratio=pipeline_job_test_train_ratio,\n",
        "    )\n",
        "\n",
        "    # using train_func like a python call with its own inputs\n",
        "    _ = train_component(\n",
        "        train_data=data_prep_job.outputs.train_data,  # note: using outputs from previous step\n",
        "        test_data=data_prep_job.outputs.test_data,  # note: using outputs from previous step\n",
        "        train_labels=data_prep_job.outputs.train_labels,  # note: using outputs from previous step\n",
        "        test_labels=data_prep_job.outputs.test_labels,  # note: using outputs from previous step\n",
        "        registered_model_name=pipeline_job_registered_model_name,\n",
        "    )\n",
        "\n",
        "    # a pipeline returns a dictionary of outputs\n",
        "    # keys will code for the pipeline output identifier\n",
        "    return {\n",
        "        \"pipeline_job_train_data\": data_prep_job.outputs.train_data,\n",
        "        \"pipeline_job_test_data\": data_prep_job.outputs.test_data,\n",
        "        \"pipeline_job_train_labels\": data_prep_job.outputs.train_labels,\n",
        "        \"pipeline_job_test_labels\": data_prep_job.outputs.test_labels,\n",
        "    }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 108,
      "metadata": {
        "attributes": {
          "classes": [
            "Python"
          ],
          "id": ""
        },
        "gather": {
          "logged": 1709651519809
        },
        "name": "registered_model_name"
      },
      "outputs": [],
      "source": [
        "registered_model_name = \"project_model\"\n",
        "\n",
        "# Let's instantiate the pipeline with the parameters of our choice\n",
        "pipeline = project_training_pipeline(\n",
        "    pipeline_job_data_input=Input(type=\"uri_file\", path=my_data.path),\n",
        "    pipeline_job_test_train_ratio=0.25,\n",
        "    pipeline_job_registered_model_name=registered_model_name,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Submit the job"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 109,
      "metadata": {
        "gather": {
          "logged": 1709651523200
        },
        "name": "returned_job"
      },
      "outputs": [],
      "source": [
        "# submit the pipeline job\n",
        "pipeline_job = ml_client.jobs.create_or_update(\n",
        "    pipeline,\n",
        "    # Project's name\n",
        "    experiment_name=\"e2e_registered_components\",\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 110,
      "metadata": {
        "gather": {
          "logged": 1709651594365
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "RunId: salmon_raisin_qxwjv5pxw1\n",
            "Web View: https://ml.azure.com/runs/salmon_raisin_qxwjv5pxw1?wsid=/subscriptions/32787e59-b8b4-4923-89db-73cff7a82fbf/resourcegroups/capstone-project/workspaces/capstone-project-workspace\n",
            "\n",
            "Streaming logs/azureml/executionlogs.txt\n",
            "========================================\n",
            "\n",
            "[2024-03-05 15:12:04Z] Submitting 1 runs, first five are: ae93ae40:225630be-2cc6-4316-aef0-79bd6baee9bb\n",
            "[2024-03-05 15:12:06Z] Completing processing run id 225630be-2cc6-4316-aef0-79bd6baee9bb.\n",
            "[2024-03-05 15:12:07Z] Submitting 1 runs, first five are: d10343f3:2de057ef-bbea-4edd-98f8-f33f05506316\n",
            "[2024-03-05 15:13:07Z] Completing processing run id 2de057ef-bbea-4edd-98f8-f33f05506316.\n",
            "\n",
            "Execution Summary\n",
            "=================\n",
            "RunId: salmon_raisin_qxwjv5pxw1\n",
            "Web View: https://ml.azure.com/runs/salmon_raisin_qxwjv5pxw1?wsid=/subscriptions/32787e59-b8b4-4923-89db-73cff7a82fbf/resourcegroups/capstone-project/workspaces/capstone-project-workspace\n",
            "\n"
          ]
        }
      ],
      "source": [
        "ml_client.jobs.stream(pipeline_job.name)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "You can track the progress of your pipeline, by using the link generated in the cell above or in this notebook using the following code:\n",
        "```python\n",
        "    ml_client.jobs.stream(pipeline_job.name)\n",
        "```\n",
        "\n",
        "When you select on each component, you'll see more information about the results of that component. \n",
        "There are two important parts to look for at this stage:\n",
        "* `Outputs+logs` > `user_logs` > `std_log.txt`\n",
        "This section shows the script run sdtout."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Deploy the model as an online endpoint"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Create a new online endpoint"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "attributes": {
          "classes": [
            "Python"
          ],
          "id": ""
        },
        "gather": {
          "logged": 1709638879519
        },
        "name": "online_endpoint_name"
      },
      "outputs": [],
      "source": [
        "import uuid\n",
        "\n",
        "# Creating a unique name for the endpoint\n",
        "online_endpoint_name = \"credit-endpoint-\" + str(uuid.uuid4())[:8]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "attributes": {
          "classes": [
            "Python"
          ],
          "id": ""
        },
        "gather": {
          "logged": 1709638975137
        },
        "name": "endpoint"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Endpint credit-endpoint-fb7ba6dd provisioning state: Succeeded\n"
          ]
        }
      ],
      "source": [
        "from azure.ai.ml.entities import (\n",
        "    ManagedOnlineEndpoint,\n",
        "    ManagedOnlineDeployment,\n",
        "    Model,\n",
        "    Environment,\n",
        ")\n",
        "\n",
        "# create an online endpoint\n",
        "endpoint = ManagedOnlineEndpoint(\n",
        "    name=online_endpoint_name,\n",
        "    description=\"this is an online endpoint\",\n",
        "    auth_mode=\"key\",\n",
        "    tags={\n",
        "        \"training_dataset\": \"credit_defaults\",\n",
        "        \"model_type\": \"sklearn.GradientBoostingClassifier\",\n",
        "    },\n",
        ")\n",
        "\n",
        "endpoint_result = ml_client.begin_create_or_update(endpoint).result()\n",
        "\n",
        "print(\n",
        "    f\"Endpint {endpoint_result.name} provisioning state: {endpoint_result.provisioning_state}\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "attributes": {
          "classes": [
            "Python"
          ],
          "id": ""
        },
        "gather": {
          "logged": 1709638975347
        },
        "name": "update-endpoint"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Endpint \"credit-endpoint-fb7ba6dd\" with provisioning state \"Succeeded\" is retrieved\n"
          ]
        }
      ],
      "source": [
        "endpoint = ml_client.online_endpoints.get(name=online_endpoint_name)\n",
        "\n",
        "print(\n",
        "    f'Endpint \"{endpoint.name}\" with provisioning state \"{endpoint.provisioning_state}\" is retrieved'\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Deploy the model to the endpoint"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "attributes": {
          "classes": [
            "Python"
          ],
          "id": ""
        },
        "gather": {
          "logged": 1709639309542
        },
        "name": "latest_model_version"
      },
      "outputs": [],
      "source": [
        "# Let's pick the latest version of the model\n",
        "latest_model_version = max(\n",
        "    [int(m.version) for m in ml_client.models.list(name=registered_model_name)]\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "gather": {
          "logged": 1709640470960
        },
        "name": "model"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Check: endpoint credit-endpoint-fb7ba6dd exists\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "........................................................................Deployment blue provisioning state: Succeeded\n"
          ]
        }
      ],
      "source": [
        "# picking the model to deploy. Here we use the latest version of our registered model\n",
        "model = ml_client.models.get(name=registered_model_name, version=latest_model_version)\n",
        "\n",
        "\n",
        "# create an online deployment.\n",
        "blue_deployment = ManagedOnlineDeployment(\n",
        "    name=\"blue\",\n",
        "    endpoint_name=online_endpoint_name,\n",
        "    model=model,\n",
        "    instance_type=\"Standard_E2s_v3\",\n",
        "    instance_count=1,\n",
        ")\n",
        "\n",
        "blue_deployment_results = ml_client.online_deployments.begin_create_or_update(\n",
        "    blue_deployment\n",
        ").result()\n",
        "\n",
        "print(\n",
        "    f\"Deployment {blue_deployment_results.name} provisioning state: {blue_deployment_results.provisioning_state}\"\n",
        ")"
      ]
    }
  ],
  "metadata": {
    "categories": [
      "SDK v2",
      "tutorials"
    ],
    "celltoolbar": "Edit Metadata",
    "description": {
      "description": "Create production ML pipelines with Python SDK v2 in a Jupyter notebook"
    },
    "kernel_info": {
      "name": "python310-sdkv2"
    },
    "kernelspec": {
      "display_name": "Python 3.10 - SDK v2",
      "language": "python",
      "name": "python310-sdkv2"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.11"
    },
    "microsoft": {
      "host": {
        "AzureML": {
          "notebookHasBeenCompleted": true
        }
      },
      "ms_spell_check": {
        "ms_spell_check_language": "en"
      }
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    },
    "vscode": {
      "interpreter": {
        "hash": "78d13900fb236b5ce4c0b952de9b1270f1802c447ee20a989ea10ffd935aa825"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 1
}
