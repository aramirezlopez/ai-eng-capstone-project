{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from operator import add\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from gensim.models import Word2Vec\n",
    "from nltk.tokenize import word_tokenize\n",
    "import gensim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word2Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we will train a word2vec model and save the processed data to avoid having to run this every time.\n",
    "\n",
    "We will follow the 300 features we established in the ComparingPretrainedModels notebook\n",
    "\n",
    "Steps:\n",
    "\n",
    "1. Model training\n",
    "2. We will pass each token for each row of the dataset to the model\n",
    "3. After having all the tokens to vectors, we will aggregate those vectors with a mean to have a text vector representation\n",
    "\n",
    "The train and test index will be saved to keep the same in the ComparingPretrainedModels. More explained at that notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('processed_text_with_all.csv')\n",
    "df = df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(df.Text, df.Labels, test_size=0.15, random_state=99)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The classes has been divided keeping the proportion of classes in both sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0    0.843087\n",
       "0.0    0.156913\n",
       "Name: Labels, dtype: float64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.value_counts() / y_train.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0    0.843359\n",
       "0.0    0.156641\n",
       "Name: Labels, dtype: float64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test.value_counts() / y_test.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = X_train.apply(word_tokenize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(143477739, 308985210)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v = Word2Vec(min_count=1,\n",
    "               window=10,\n",
    "               vector_size=300,\n",
    "               sample=6e-5,\n",
    "               alpha=0.03,\n",
    "               min_alpha=0.0007,\n",
    "               negative=20)\n",
    "w2v.build_vocab(sentences, progress_per=100)\n",
    "w2v.train(sentences, total_examples=w2v.corpus_count, epochs=30, report_delay=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v.save(\"w2v_30epochs_window10_size300.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word2vec_get_vector(w2v, word):\n",
    "    try:\n",
    "        word = w2v.wv.get_vector(word)\n",
    "    except KeyError:\n",
    "        word = [0] * model.wv.vector_size\n",
    "    return word\n",
    "\n",
    "\n",
    "def ind_sentence_process(sentence, w2v):\n",
    "    sent_vec = word2vec_get_vector(w2v, sentence[0])\n",
    "    for word in sentence[1:]:\n",
    "        sent_vec = list(map(add, sent_vec, word2vec_get_vector(w2v, word)))\n",
    "    return list(map(lambda x: x / len(sentence), sent_vec))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_to_vec(X, model):\n",
    "    X_wv = X.apply(word_tokenize).apply(ind_sentence_process, w2v=model)\n",
    "    X_wv = pd.DataFrame(X_wv.tolist())\n",
    "    return X_wv\n",
    "\n",
    "\n",
    "def text_to_vec_workflow(data_dict, model):\n",
    "    data_wv_dict = {}\n",
    "    for dataset_type, dataset in data_dict.items():\n",
    "        data_wv = text_to_vec(dataset, model)\n",
    "        data_wv.index = dataset.index\n",
    "        data_wv.to_csv(f'w2v_window10_{dataset_type}_data.csv')\n",
    "        data_wv_dict[dataset_type] = data_wv\n",
    "    return data_wv_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load pre-trained Word2Vec model.\n",
    "model = gensim.models.Word2Vec.load(\"w2v_30epochs_window10_size300.model\")\n",
    "data_dict = {'train': X_train, 'test': X_test}  # , 'val': X_val\n",
    "\n",
    "data_wv_dict = text_to_vec_workflow(data_dict, model)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "academy",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
